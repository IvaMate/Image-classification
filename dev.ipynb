{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivam/Image_project/venv_image/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ViTForImageClassification, pipeline, ViTFeatureExtractor, Trainer, TrainingArguments\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = 'data/raw'       \n",
    "processed_data_dir = 'data/processed'\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1 # and test ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ['bird', 'cat', 'dog']:\n",
    "    category_path = os.path.join(raw_data_dir, category)\n",
    "    files = os.listdir(category_path) #ls\n",
    "    random.shuffle(files)  \n",
    "\n",
    "    total_files = len(files)\n",
    "    train_count = int(total_files * train_ratio)\n",
    "    val_count = int(total_files * val_ratio)\n",
    "\n",
    "    train_files = files[:train_count]\n",
    "    val_files = files[train_count:train_count + val_count]\n",
    "    test_files = files[train_count + val_count:]\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.move(os.path.join(category_path, file), os.path.join(processed_data_dir, 'train', category, file))\n",
    "    for file in val_files:\n",
    "        shutil.move(os.path.join(category_path, file), os.path.join(processed_data_dir, 'validation', category, file))\n",
    "    for file in test_files:\n",
    "        shutil.move(os.path.join(category_path, file), os.path.join(processed_data_dir, 'test', category, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': image,  \n",
    "            'labels': torch.tensor(label, dtype=torch.long) \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(root_dir=os.path.join(processed_data_dir, 'train'), transform=transform)\n",
    "val_dataset = CustomImageDataset(root_dir=os.path.join(processed_data_dir, 'validation'), transform=transform)\n",
    "test_dataset = CustomImageDataset(root_dir=os.path.join(processed_data_dir, 'test'), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 334\n",
      "Validation batches: 42\n",
      "Test batches: 42\n"
     ]
    }
   ],
   "source": [
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')\n",
    "print(f'Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 10675\n",
      "Validation images: 1333\n",
      "Test images: 1336\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the number of images in each dataset\n",
    "print(f'Training images: {len(train_dataset)}')\n",
    "print(f'Validation images: {len(val_dataset)}')\n",
    "print(f'Test images: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- testing model before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/processed/test/bird/0.jpg - Predicted Label: dog with confidence 0.94\n",
      "./data/processed/test/bird/15.jpg - Predicted Label: dog with confidence 0.92\n",
      "./data/processed/test/bird/41.jpg - Predicted Label: dog with confidence 0.91\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    \"./data/processed/test/bird/0.jpg\",\n",
    "    \"./data/processed/test/bird/15.jpg\",\n",
    "    \"./data/processed/test/bird/41.jpg\"\n",
    "]\n",
    "\n",
    "# Initialize the image classification pipeline\n",
    "classifier = pipeline(task=\"image-classification\", model=\"akahana/vit-base-cats-vs-dogs\", device=0)\n",
    "\n",
    "# Loop through each image path\n",
    "for img_path in image_paths:\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    # Run inference\n",
    "    predictions = classifier(image)\n",
    "    \n",
    "    # Print out the image path and predicted label\n",
    "    print(f\"{img_path} - Predicted Label: {predictions[0]['label']} with confidence {predictions[0]['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivam/Image_project/venv_image/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at akahana/vit-base-cats-vs-dogs and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"_name_or_path\": \"akahana/vit-base-cats-vs-dogs\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"akahana/vit-base-cats-vs-dogs\")\n",
    "model = ViTForImageClassification.from_pretrained(\"akahana/vit-base-cats-vs-dogs\", num_labels=3,ignore_mismatched_sizes=True)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"_name_or_path\": \"akahana/vit-base-cats-vs-dogs\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"bird\",\n",
      "    \"1\": \"cat\",\n",
      "    \"2\": \"dog\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"bird\": \"0\",\n",
      "    \"cat\": \"1\",\n",
      "    \"dog\": \"2\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    \"0\": \"bird\", \n",
    "    \"1\": \"cat\", \n",
    "    \"2\": \"dog\" \n",
    "  }\n",
    "label2id = {\n",
    "    \"bird\": \"0\",\n",
    "    \"cat\": \"1\",\n",
    "    \"dog\": \"2\"\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- move model to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    per_device_train_batch_size=batch_size,              \n",
    "    per_device_eval_batch_size=batch_size,               \n",
    "    num_train_epochs=num_epochs,                         \n",
    "    eval_strategy=\"epoch\",                  \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,                                  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 08:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.135551</td>\n",
       "      <td>{'accuracy': 0.9639909977494373}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=334, training_loss=0.19999324775741487, metrics={'train_runtime': 514.8141, 'train_samples_per_second': 20.736, 'train_steps_per_second': 0.649, 'total_flos': 8.272344033096192e+17, 'train_loss': 0.19999324775741487, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/checkpoint-334/preprocessor_config.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create preprocessor_config.json\n",
    "feature_extractor.save_pretrained(\"./model/checkpoint-334\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivam/Image_project/venv_image/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/processed/test/bird/0.jpg - Predicted Label: bird with confidence 0.94\n",
      "./data/processed/test/bird/15.jpg - Predicted Label: bird with confidence 0.93\n",
      "./data/processed/test/bird/41.jpg - Predicted Label: bird with confidence 0.93\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to your fine-tuned model\n",
    "model_path = \"./model/checkpoint-334\"\n",
    "\n",
    "# Initialize the image classification pipeline with your fine-tuned model\n",
    "classifier = pipeline(task=\"image-classification\", model=model_path, device=0)  # Use device=-1 for CPU\n",
    "\n",
    "# List of image paths to classify\n",
    "image_paths = [\n",
    "    \"./data/processed/test/bird/0.jpg\",\n",
    "    \"./data/processed/test/bird/15.jpg\",\n",
    "    \"./data/processed/test/bird/41.jpg\"\n",
    "]\n",
    "\n",
    "# Loop through each image path\n",
    "for img_path in image_paths:\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    # Run inference\n",
    "    predictions = classifier(image)\n",
    "    \n",
    "    # Print out the image path and predicted label\n",
    "    print(f\"{img_path} - Predicted Label: {predictions[0]['label']} with confidence {predictions[0]['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
